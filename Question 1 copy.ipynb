{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c2c909ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion à WRDS...\n",
      "Loading library list...\n",
      "Loading library list...\n",
      "Done\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_21040\\2067690016.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_comp = pd.concat(compustat_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Données extraites et formatées : prix quotidiens et EPS trimestriel (merge direct, sans backward fill).\n",
      "        date ticker  price  epspxq\n",
      "0 2003-01-02    MMC  47.59    <NA>\n",
      "1 2003-01-03    MMC  47.45    <NA>\n",
      "2 2003-01-06    MMC  48.82    <NA>\n",
      "3 2003-01-07    MMC  48.73    <NA>\n",
      "4 2003-01-08    MMC  47.84    <NA>\n",
      "\n",
      "✓ Fichier CSV sauvegardé : c:\\Users\\grego\\OneDrive\\Documents\\GitHub\\Devoir-2\\WRDS_extract.csv\n"
     ]
    }
   ],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print('Connexion à WRDS...')\n",
    "db = wrds.Connection(wrds_username='gregcouts1')\n",
    "\n",
    "# Paramètres\n",
    "start_date = '2003-01-01'\n",
    "end_date = '2023-12-31'\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# 1. Extraire tous les tickers S&P 500 avec market-cap et secteur, présents sur toute la période\n",
    "sp500_query = f\"\"\"\n",
    "    SELECT DISTINCT a.permno, b.ticker, b.shrcd, b.exchcd, b.namedt, b.nameenddt, b.siccd, b.ncusip\n",
    "    FROM crsp.msp500list a\n",
    "    JOIN crsp.stocknames b ON a.permno = b.permno\n",
    "    WHERE a.start <= '{start_date}'\n",
    "    AND a.ending >= '{end_date}'\n",
    "\"\"\"\n",
    "sp500_df = db.raw_sql(sp500_query)\n",
    "\n",
    "# Récupérer market-cap mensuel pour chaque permno\n",
    "mcap_query = f\"\"\"\n",
    "    SELECT permno, date, prc, shrout, abs(prc)*shrout*0.001 as mcap_musd\n",
    "    FROM crsp.msf\n",
    "    WHERE date BETWEEN '{start_date}' AND '{end_date}'\n",
    "\"\"\"\n",
    "mcap_df = db.raw_sql(mcap_query, date_cols=['date'])\n",
    "mcap_df = mcap_df.sort_values(['permno','date'])\n",
    "mcap_last = mcap_df.groupby('permno').last().reset_index()\n",
    "\n",
    "# Merge market-cap et tickers\n",
    "sp500_df = pd.merge(sp500_df, mcap_last[['permno','mcap_musd']], on='permno', how='left')\n",
    "\n",
    "# Exclure les firmes sous le 30e percentile de market-cap NYSE\n",
    "nyse_permnos = sp500_df[sp500_df['exchcd']==1]['permno'].unique()\n",
    "nyse_mcaps = sp500_df[sp500_df['permno'].isin(nyse_permnos)]['mcap_musd'].dropna()\n",
    "mcap_cutoff = np.percentile(nyse_mcaps, 30)\n",
    "sp500_df = sp500_df[sp500_df['mcap_musd'] >= mcap_cutoff]\n",
    "\n",
    "# Exclure les industries Fama-French (real estate, coal, steel, mines, oil, gold)\n",
    "excluded_sic = [65, 12, 33, 10, 13, 10] # Real estate, coal, steel, mines, oil, gold (approximate SIC codes)\n",
    "sp500_df = sp500_df[~sp500_df['siccd'].astype(str).str[:2].isin([str(s) for s in excluded_sic])]\n",
    "\n",
    "# 2. Extraction Compustat (EPS trimestriel) pour tous les tickers restants\n",
    "compustat_data = []\n",
    "for ticker in sp500_df['ticker'].unique():\n",
    "    eps_query = f\"\"\"\n",
    "        SELECT datadate, epspxq, tic\n",
    "        FROM comp.fundq\n",
    "        WHERE tic = '{ticker}'\n",
    "        AND indfmt = 'INDL'\n",
    "        AND datafmt = 'STD'\n",
    "        AND popsrc = 'D'\n",
    "        AND consol = 'C'\n",
    "        AND datadate >= '{start_date}'\n",
    "        AND datadate <= '{end_date}'\n",
    "        ORDER BY datadate\n",
    "    \"\"\"\n",
    "    eps_df = db.raw_sql(eps_query, date_cols=['datadate'])\n",
    "    if eps_df.empty:\n",
    "        continue\n",
    "    eps_df['ticker'] = ticker\n",
    "    compustat_data.append(eps_df)\n",
    "if compustat_data:\n",
    "    df_comp = pd.concat(compustat_data, ignore_index=True)\n",
    "else:\n",
    "    df_comp = pd.DataFrame(columns=['datadate','epspxq','ticker'])\n",
    "\n",
    "# 3. Extraction CRSP (prix quotidiens) pour ces tickers\n",
    "crsp_data = []\n",
    "for ticker in sp500_df['ticker'].unique():\n",
    "    permnos = sp500_df[sp500_df['ticker']==ticker]['permno']\n",
    "    if permnos.empty:\n",
    "        continue\n",
    "    permno = permnos.iloc[0]\n",
    "    price_query = f\"\"\"\n",
    "        SELECT date, prc\n",
    "        FROM crsp.dsf\n",
    "        WHERE permno = {permno}\n",
    "        AND date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        ORDER BY date\n",
    "    \"\"\"\n",
    "    price_df = db.raw_sql(price_query, date_cols=['date'])\n",
    "    if price_df.empty:\n",
    "        continue\n",
    "    price_df['price'] = price_df['prc'].abs()\n",
    "    price_df['ticker'] = ticker\n",
    "    crsp_data.append(price_df)\n",
    "if crsp_data:\n",
    "    df_crsp = pd.concat(crsp_data, ignore_index=True)\n",
    "else:\n",
    "    df_crsp = pd.DataFrame(columns=['date','prc','price','ticker'])\n",
    "\n",
    "# Merge direct des datasets sur ticker et date (sans backward fill EPS)\n",
    "df_crsp['date'] = pd.to_datetime(df_crsp['date'], errors='coerce')\n",
    "df_comp['datadate'] = pd.to_datetime(df_comp['datadate'], errors='coerce')\n",
    "df_crsp['ticker'] = df_crsp['ticker'].astype(str)\n",
    "df_comp['ticker'] = df_comp['ticker'].astype(str)\n",
    "\n",
    "merged = pd.merge(df_crsp, df_comp, left_on=['ticker','date'], right_on=['ticker','datadate'], how='left')\n",
    "\n",
    "final = merged[['date','ticker','price','epspxq']].copy()\n",
    "output_path = os.path.join(os.getcwd(), 'WRDS_extract.csv')\n",
    "final.to_csv(output_path, index=False)\n",
    "print(\"\\n✓ Données extraites et formatées : prix quotidiens et EPS trimestriel (merge direct, sans backward fill).\")\n",
    "print(final.head())\n",
    "print(f\"\\n✓ Fichier CSV sauvegardé : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4a50bd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion WRDS fermée avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Fermer explicitement la connexion WRDS si elle existe\n",
    "try:\n",
    "    db.close()\n",
    "    print('Connexion WRDS fermée avec succès.')\n",
    "except Exception as e:\n",
    "    print(f'Erreur lors de la fermeture de la connexion WRDS : {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e4b91060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_21040\\2705461045.py:8: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  df['BPA'] = df.groupby('ticker')['epspxq'].fillna(method='ffill')\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_21040\\2705461045.py:8: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['BPA'] = df.groupby('ticker')['epspxq'].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Nettoyage, forward fill, exclusion BPA=0, renommage et export terminés. Fichier sauvegardé : stock_random.csv\n",
      "         date ticker  price   BPA  P/B glissant\n",
      "20 2003-01-31      A  16.48 -0.24    -68.666667\n",
      "21 2003-02-03      A  16.49 -0.24    -68.708333\n",
      "22 2003-02-04      A  16.30 -0.24    -67.916667\n",
      "23 2003-02-05      A  16.32 -0.24    -68.000000\n",
      "24 2003-02-06      A  12.26 -0.24    -51.083333\n"
     ]
    }
   ],
   "source": [
    "# Forward fill EPS from WRDS_extract.csv, nettoyage, suppression des lignes sans EPS filled, exclusion EPS=0, renommage colonnes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "csv_path = 'WRDS_extract.csv'\n",
    "df = pd.read_csv(csv_path, parse_dates=['date'])\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "# Forward fill EPS pour chaque ticker\n",
    "df['BPA'] = df.groupby('ticker')['epspxq'].fillna(method='ffill')\n",
    "# Supprimer les stocks qui n'ont jamais de BPA (même après ffill)\n",
    "tickers_with_bpa = df.groupby('ticker')['BPA'].apply(lambda x: x.notna().any())\n",
    "df = df[df['ticker'].isin(tickers_with_bpa[tickers_with_bpa].index)]\n",
    "# Supprimer les lignes sans BPA\n",
    "df = df[df['BPA'].notna()]\n",
    "# Exclure les stocks à BPA = 0\n",
    "df = df[df['BPA'] != 0]\n",
    "# Calculer P/B glissant\n",
    "df['P/B glissant'] = np.where(df['BPA'].notna(), df['price']/df['BPA'], np.nan)\n",
    "# Sauver le résultat (stock_random.csv) sans epspxq\n",
    "output_path = 'stock_random.csv'\n",
    "df[['date','ticker','price','BPA','P/B glissant']].to_csv(output_path, index=False)\n",
    "print(f\"✓ Nettoyage, forward fill, exclusion BPA=0, renommage et export terminés. Fichier sauvegardé : {output_path}\")\n",
    "print(df[['date','ticker','price','BPA','P/B glissant']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "59660cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tirage réplicable de 50 stocks terminé. Fichier sauvegardé : stock_draw_50.csv\n",
      "Tickers tirés au sort (50): ['SHW' 'CAT' 'PA' 'C' 'SRE' 'WMB' 'XEL' 'DHR' 'MS' 'ELV' 'AON' 'USB' 'RTX'\n",
      " 'CTAS' 'GD' 'JNJ' 'BHI' 'NB' 'HES' 'MU' 'AZO' 'APD' 'CCC' 'AXP' 'ALL'\n",
      " 'AEP' 'INTU' 'MCRN' 'HON' 'EA' 'COST' 'ECL' 'TJX' 'NKE' 'GE' 'SD' 'ABT'\n",
      " 'LOW' 'SBUX' 'UPS' 'KR' 'NSP' 'BKR' 'SYY' 'SCHW' 'MCO' 'FCX' 'CL' 'PG'\n",
      " 'BDX']\n",
      "DataFrame filtré shape: (245675, 5)\n"
     ]
    }
   ],
   "source": [
    "# Tirage aléatoire réplicable de 50 stocks, création d'un 3e DataFrame et export CSV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('stock_random.csv', parse_dates=['date'])\n",
    "np.random.seed(42)\n",
    "tickers = df['ticker'].drop_duplicates().values\n",
    "n_draw = min(50, len(tickers))\n",
    "tickers_draw = np.random.choice(tickers, size=n_draw, replace=False)\n",
    "df_draw = df[df['ticker'].isin(tickers_draw)].copy()\n",
    "output_draw_path = 'stock_draw_50.csv'\n",
    "df_draw.to_csv(output_draw_path, index=False)\n",
    "print(f\"✓ Tirage réplicable de {n_draw} stocks terminé. Fichier sauvegardé : {output_draw_path}\")\n",
    "print(f\"Tickers tirés au sort ({n_draw}): {tickers_draw}\")\n",
    "print(f\"DataFrame filtré shape: {df_draw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f3eb71cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n",
      "Done\n",
      "✓ Extraction, fusion et export CSV avec la colonne ibes_est terminés.\n",
      "        date ticker  price   BPA  P/B glissant    fpedats  ibes_est\n",
      "0 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31     0.545\n",
      "1 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31      0.53\n",
      "2 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31      0.53\n",
      "3 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31     0.575\n",
      "4 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31     0.575\n",
      "✓ Extraction, fusion et export CSV avec la colonne ibes_est terminés.\n",
      "        date ticker  price   BPA  P/B glissant    fpedats  ibes_est\n",
      "0 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31     0.545\n",
      "1 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31      0.53\n",
      "2 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31      0.53\n",
      "3 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31     0.575\n",
      "4 2003-03-31    ABT  37.61  0.47     80.021277 2003-03-31     0.575\n"
     ]
    }
   ],
   "source": [
    "# Extraction et fusion des prévisions d'EPS IBES pour les 50 stocks tirés au sort (moyenne highest/lowest) + export CSV\n",
    "import wrds\n",
    "db = wrds.Connection(wrds_username='gregcouts1')\n",
    "df_draw = pd.read_csv('stock_draw_50.csv', parse_dates=['date'])\n",
    "tickers_draw = df_draw['ticker'].drop_duplicates().tolist()\n",
    "# Récupérer les prévisions d'EPS IBES pour chaque ticker\n",
    "ibes_data = []\n",
    "for ticker in tickers_draw:\n",
    "    ibes_query = f\"\"\"\n",
    "        SELECT ticker, fpedats, fpi, highest, lowest\n",
    "        FROM ibes.statsum_epsus\n",
    "        WHERE ticker = '{ticker}'\n",
    "        AND fpedats >= '2003-01-01'\n",
    "        AND fpedats <= '2023-12-31'\n",
    "        ORDER BY fpedats\n",
    "    \"\"\"\n",
    "    ibes_df = db.raw_sql(ibes_query, date_cols=['fpedats'])\n",
    "    if not ibes_df.empty:\n",
    "        ibes_data.append(ibes_df)\n",
    "if ibes_data:\n",
    "    df_ibes = pd.concat(ibes_data, ignore_index=True)\n",
    "    df_ibes['ibes_est'] = df_ibes[['highest','lowest']].mean(axis=1)\n",
    "else:\n",
    "    df_ibes = pd.DataFrame(columns=['ticker','fpedats','fpi','highest','lowest','ibes_est'])\n",
    "# Fusionner la prévision IBES avec le DataFrame tiré\n",
    "df_draw = df_draw.merge(df_ibes[['ticker','fpedats','ibes_est']], left_on=['ticker','date'], right_on=['ticker','fpedats'], how='left')\n",
    "# Exporter le DataFrame enrichi dans le CSV\n",
    "output_draw_path = 'stock_draw_50.csv'\n",
    "df_draw.to_csv(output_draw_path, index=False)\n",
    "print(\"✓ Extraction, fusion et export CSV avec la colonne ibes_est terminés.\")\n",
    "print(df_draw.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
